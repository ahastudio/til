# Async/Await on GPU

원문: <https://www.vectorware.com/blog/async-await-on-gpu/>

## 핵심 아이디어

`async/await`를 CPU I/O 대기 최적화 문법으로만 보지 않고, GPU 작업 스케줄링
관점으로 확장해서 보는 시각이 중요하다.

일반적인 GPU 파이프라인에서는 커널 실행, 메모리 복사, 동기화 지점이 섞이며,
개발자는 성능과 코드 가독성 사이에서 자주 트레이드오프를 겪는다.

`await`를 "블로킹"이 아니라 "의존성 충족 시점까지의 제어권 반환"으로 해석하면,
GPU 연산 파이프라인을 더 명시적으로 구성할 수 있다.

## 인사이트

### 1) "커널 호출"이 아니라 "작업 그래프"로 모델링

실무에서는 개별 커널 호출 최적화보다, 작업 간 의존성을 DAG로 모델링하는 것이
효율적일 때가 많다.

- CPU 전처리
- GPU 커널 A
- GPU 커널 B
- CPU 후처리

이 흐름을 Future 체인으로 표현하면, 병렬 가능한 경로와 직렬 경로를 코드에서
바로 읽을 수 있다.

### 2) 동기화 지점을 최소화하고, 경계를 명확히 하라

성능 저하는 대개 "GPU가 느려서"가 아니라 불필요한 동기화에서 발생한다.

- 매 단계 `await`로 즉시 결과를 꺼내지 않는다.
- 가능한 한 마지막 경계에서만 결과를 회수한다.
- 중간 단계는 디바이스 상에 유지한다.

핵심은 `await` 위치를 비즈니스 로직 단위가 아니라 데이터 경계 단위로 두는 것이다.

### 3) 스트림/큐 단위의 동시성 전략을 먼저 설계

`async/await`를 도입해도 기본 실행 큐가 하나면 체감 이점이 작다.

- 고우선순위: 짧은 상호작용성 작업
- 저우선순위: 긴 배치 작업

처럼 스트림을 분리해 두면, tail latency를 낮추면서 전체 처리량을 유지하기 좋다.

### 4) "오류 모델"을 CPU와 분리해서 다뤄라

GPU 에러는 지연 보고되는 경우가 많다.

- 커널 제출 시점
- 동기화 시점
- 메모리 접근 시점

오류 발생 지점이 다를 수 있으므로, `Result` 수집 전략을 단계별로 정의해야
디버깅 비용이 줄어든다.

### 5) 관측성(Observability)을 함께 설계하라

`async` 구조는 성능 개선 여지를 만들지만, 동시에 병목을 숨길 수 있다.

- 구간별 타임스탬프
- 큐 대기 시간
- 디바이스 실행 시간
- 호스트-디바이스 전송량

을 최소 단위로 기록해 두면, "왜 느린가"를 감으로 추정하지 않아도 된다.

## 팀 적용 체크리스트

- GPU 경계를 기준으로 `await` 위치를 재배치했는가?
- 결과 회수 동기화를 최소화했는가?
- 스트림/큐 분리 전략이 SLA와 연결되어 있는가?
- 실패 지점을 제출/실행/동기화로 분리해 로깅하는가?
- 성능 측정이 커널 시간 + 전송 시간 + 대기 시간을 포함하는가?

## 실전 도입 순서

1. 현재 파이프라인의 동기화 지점을 모두 시각화한다.
2. `await`를 최소 경계로 모아서 재구성한다.
3. 스트림 분리 후 p95/p99 지연 시간을 다시 측정한다.
4. 오류 모델과 로깅 포맷을 표준화한다.
5. 최적화 전후를 회귀 테스트와 벤치마크로 고정한다.

## 함께 보면 좋은 주제

- CUDA Stream / Event 기반 비동기 실행
- Vulkan Compute Queue 동기화 모델
- Rust `Future` 기반 작업 그래프 추상화
- 배치 크기(Batch Size)와 GPU 점유율(Occupancy)의 상관관계
