# Cosmologically Unique ID

<https://jasonfantl.com/posts/Universal-Unique-IDs/>

<https://news.hada.io/topic?id=26821>

UUID는 지구에서 쓰기엔 충분하다.
그런데 인류가 은하 너머로 퍼져나간다면?
중앙 권한 없이, 우주 어디서든 충돌 없는 ID를
만들려면 몇 비트가 필요할까?

## 랜덤 ID

가장 단순한 방법은 매번 랜덤 숫자를 뽑는 것이다.
중앙 서버도, 조율도 필요 없다.
문제는 두 기기가 같은 숫자를 뽑을 수 있다는 점인데,
숫자 범위를 충분히 크게 잡으면
충돌 확률을 "사실상 0"으로 만들 수 있다.

운석에 맞을 확률은 0이 아니다.
하지만 지구상 모든 사람이 지금 이 순간
동시에 운석에 맞을 확률은?
0은 아니지만 사실상 불가능하다.
ID 충돌 확률을 바로 그 수준으로 낮출 수 있다.

## 얼마나 커야 하는가

"Universal Limits on Computation" 논문에 따르면,
우주 전체를 최대 효율 컴퓨터(Computronium)로
바꿔도 열 죽음(Heat Death)까지
약 10^120번의 연산만 가능하다.

생일 역설(Birthday Paradox)로 계산하면,
n개의 ID를 랜덤 생성할 때 충돌이 기대되는
ID 공간 크기는 n^2이다.

모든 연산이 새 ID를 생성한다고 가정하면:

| 시나리오                                | 필요 비트 |
|-----------------------------------------|-----------|
| Computronium 전체 연산 (10^120)         | ~798      |
| 관측 가능한 우주의 모든 원자 (10^80)    | ~532      |
| 우주 전체 질량을 1g 나노봇으로 (10^57)  | ~372      |
| 현행 UUID v4                            | 122       |

798비트면 모든 장치, 모든 마이크로칩,
모든 키 입력, 모든 시계의 모든 틱,
모든 별과 모든 원자에 ID를 부여해도
충돌이 기대되지 않는다.

## 결정론적 방식의 탐구

"사실상 0"이 아니라 "진짜 0"을 원한다면?
확률적 접근을 거부하고 이론적으로 충돌이
불가능한 결정론적(Deterministic) 방식을 탐구한다.

### Dewey 방식

중앙 컴퓨터가 카운터로 ID를 순차 할당하면
고유성은 보장된다. 하지만 먼 행성에서
중앙 서버에 접근할 수 없다면?

ID를 가진 모든 기기가 하위 ID를 할당할 수 있게
하면 된다. 위성 13이 할당한 6번째 ID는 `13.5`,
그 식민선이 만든 로봇은 `13.5.3`이 된다.
듀이 십진분류법과 유사한 계층 구조다.

확장 트리(넓게 퍼지는 경우)에서는
로그 성장하지만, 체인(한 줄로 이어지는 경우)
에서는 선형으로 커진다.

### Binary, 2-Adic Valuation, Token 방식

Binary 방식은 ID 공간을 이진 트리로 나누어
각 기기에 무한한 하위 ID 풀을 부여한다.
2-Adic Valuation은 산술적 변환으로
Binary와 동일한 성장 특성을 가진다.
Token 방식은 토큰 전달과 홉 카운트를 사용하여
체인에서 로그 성장을 달성하지만,
폭과 깊이가 함께 자라면 다시 선형이 된다.

### 결정론적 방식의 한계 증명

**어떤 결정론적 방식이든
최악의 경우 ID 길이는 선형으로 증가한다.**

n개 노드를 추가할 때 가능한 모든 할당 이력을
고려하면, 고유해야 하는 ID 수가 2^n으로 증가한다.
이를 카운터로 인코딩해도 최소 n비트가 필요하므로,
어떤 방식을 쓰든 최악의 경우 선형 성장을 피할 수
없다.

## 우주 규모 시뮬레이션

실제 인류 확장 모델을 시뮬레이션해 보면
결정론적 방식의 문제가 극명해진다.

행성 하나에서 Fitness 모델(기기별 인기도에 따른
ID 요청 분포)로 시뮬레이션하면,
Dewey 방식이 가장 좋은 성능을 보이며
평균적으로 로그 성장한다.

하지만 행성 간, 은하 간으로 확장하면:

| 규모         | Dewey 방식 ID 길이 |
|--------------|---------------------|
| 행성 하나    | ~900비트            |
| 은하 끝까지  | ~2,700,000비트      |

은하 끝까지 도달하면 약 270만 비트,
**ID 하나에 338KB**가 필요하다.
랜덤 방식의 최대 798비트(100바이트)와 비교하면
압도적으로 비효율적이다.

## HN 반론: 인과적 접촉

원문은 우주 전체의 모든 ID가 서로 충돌할 수 있다고
가정하지만, 실제로 충돌이 "의미 있으려면" 두 ID가
인과적 접촉(Causal Contact)을 해야 한다.
빛의 원뿔(Light Cone)로 제한되는 통신 범위를
고려하면, 생일 역설을 우주 전체에 적용하는 것은
문제를 과대평가하는 것이다.

이 관점에서 보면 128비트는 인류가 천 년간
만들 시스템에도 충분하고,
256비트면 우주 전체에도 과하다.

또 다른 관점: 전 인류의 데이터 총량은 아직
1요타바이트(Yottabyte)도 안 된다.
256비트 ID로 50% 충돌 확률에 도달하려면
약 10^16 요타바이트의 ID가 필요하다.
데이터센터가 수백만 번 폭발할 확률이
충돌 확률보다 높다.

## HN 반론: 신뢰할 수 없는 주체

단일 시스템이라면 단순 카운터로 충분하고,
여러 서버라면 샤딩된 카운터로 구간을 나누면 된다.
하지만 분산 환경에서 신뢰할 수 없는 주체가
ID를 생성한다면 랜덤 충돌이 아니라
악의적 충돌(Intentional Collision)이 문제이므로,
비트 수와 무관하게 검증이 필요하다.

## 인사이트

### 100바이트 vs 338KB: 완벽주의의 대가

결정론적 보장을 추구한 결과 ID 하나에
338KB가 필요해졌다. 랜덤은 100바이트.
3,000배 이상의 차이다.

이 차이는 단순한 비효율이 아니라 구조적 필연이다.
결정론적 방식은 "누가 언제 누구에게 ID를 받았는지"
라는 이력 정보를 ID 안에 인코딩해야 한다.
행성을 거치고, 은하를 건너면서
이력이 쌓이면 ID가 무한히 길어진다.
반면 랜덤은 이력이 필요 없다.
과거에 무슨 일이 있었든 상관없이
지금 여기서 숫자 하나를 뽑으면 된다.

"사실상 0"을 "진짜 0"으로 만드는 비용이
이렇게 크다. 확률을 받아들이는 것이
올바른 엔지니어링 판단이다.
소프트웨어 설계에서도 마찬가지다.
Exactly-once 보장 대신 Idempotent 설계를 택하고,
강한 일관성(Strong Consistency) 대신
결과적 일관성(Eventual Consistency)을 택하는 것은
같은 종류의 절충이다.

### 인과성이 문제를 축소한다

충돌은 두 ID가 만나야만 의미가 있다.
은하 A와 은하 B가 영영 통신하지 않으면
같은 ID를 써도 아무 문제가 없다.

원문은 지역성(Locality)을 ID 생성의 동기로 삼으면서
충돌 확률 계산에서는 무시한다.
빛의 원뿔(Light Cone)로 제한되는
실제 통신 범위를 고려하면,
생일 역설을 우주 전체에 적용하는 것은
문제를 과대평가하는 것이다.

이 관점에서 보면 128비트는 인류가 천 년간
만들 시스템에도 충분하고,
256비트면 우주 전체에도 과하다.
798비트는 최악 중의 최악이다.

이건 분산 시스템 설계에서도 동일하게 적용된다.
글로벌 유일성이 필요한가, 아니면 파티션 내
유일성이면 충분한가? 문제의 크기는
물리적 연결 범위에 달렸다.
시스템의 경계를 먼저 정의하면
필요한 ID 크기는 극적으로 줄어든다.

### 물리학이 "충분히 큰"을 증명해 준다

엔지니어가 "이 정도면 충분하다"고 말할 때
보통 직감에 의존한다. 640KB면 충분하다던
빌 게이츠의 (잘못 인용된) 말처럼,
"충분히 큰 숫자"는 보통 시간이 지나면
부족해진다.

하지만 Computronium 모델은 다르다.
우주가 수행 가능한 연산의 절대 상한
(10^120)은 물리 법칙에서 도출된 것이다.
이보다 많은 ID는 물리적으로 생성이 불가능하다.
기술이 아무리 발전해도 이 한계는 변하지 않는다.

"충분히 큰 숫자"에 물리학적 근거가 붙는
드문 경우다. 보통 엔지니어링 상한선은
현재 기술 수준에 의존하지만,
여기서는 우주의 기본 상수가 한계를 정한다.
이런 종류의 상한선은 반박할 수 없다.

### 조율 없는 고유성: 분산 시스템의 핵심

UUID가 사실상 표준이 된 이유는
합의(Consensus) 프로토콜 없이
고유성을 달성하기 때문이다.
은하 간 통신에 빛의 속도 지연이 있는
상황에서 중앙 조율은 원천적으로 불가능하다.

이건 우주적 문제만이 아니다.
마이크로서비스 간 네트워크 지연,
오프라인 우선 모바일 앱,
CRDT 기반 실시간 협업 도구,
IoT 디바이스의 간헐적 연결 —
현실의 분산 시스템도 같은 제약 아래 있다.

Paxos나 Raft 같은 합의 프로토콜은
참여자 간 통신을 전제한다.
통신이 불가능하거나 비용이 극도로 높은
환경에서는 합의 자체가 성립하지 않는다.
랜덤 ID는 이 한계를 우회한다.
조율 비용을 0으로 만드는 것이
랜덤 ID의 진짜 가치다.

Snowflake ID(Twitter, Discord)나
ULID처럼 타임스탬프를 섞는 혼합 방식도 있지만,
우주 전체가 하나의 시계에 동의하는 것은
불가능에 가깝다. 상대성 이론에 따르면
시간 자체가 관찰자마다 다르다.
결국 순수 랜덤이 가장 보편적인 해법이다.

### 결정론적 방식의 한계는 증명 가능하다

"더 똑똑한 알고리즘"을 만들면 해결되지 않을까?
그렇지 않다는 것이 수학적으로 증명된다.

n개 노드를 추가할 때 가능한 모든 할당 이력을
고려하면, 각 이력마다 다른 ID가 필요하므로
ID 후보가 2^n개로 폭발한다.
이를 카운터로 인코딩해도 최소 n비트가 필요하다.
이건 정보 이론적 하한이다.

이 증명이 흥미로운 이유는,
알고리즘의 영리함이 아니라
문제 자체의 구조가 한계를 결정한다는 점이다.
P vs NP 문제처럼, 어떤 문제는
"풀 수 있는 것의 한계" 자체가
연구 대상이 된다.

평균적으로는 로그 성장이 가능하다.
시뮬레이션에서 Random, Preferential, Fitness
모델 모두 로그 성장을 보였다.
하지만 최악의 경우를 보장할 수 없다는 점이
결정론적 방식의 근본적 약점이다.
랜덤은 이 문제를 비트 수 하나로 해결한다.

### 충돌보다 악의가 더 현실적 위협이다

UUID 충돌 확률은 데이터센터가 수백만 번
폭발할 확률보다 낮다.
전 인류의 데이터 총량은 아직
1요타바이트도 안 되는데,
256비트 ID로 50% 충돌 확률에 도달하려면
약 10^16 요타바이트의 ID가 필요하다.

진짜 위험은 신뢰할 수 없는 주체가 의도적으로
같은 ID를 생성하는 것이다.
이건 비트 수를 늘려서 막을 수 없다.
공개키를 ID로 쓰거나(DID가 이 접근),
서명 체인을 붙이는 등
별도의 검증 메커니즘이 필요하다.

원문도 이 점을 언급한다.
랜덤 ID의 경우 공개키 자체를 ID로 쓰고,
결정론적 방식에서는 부모가 자식의 공개키에
서명하여 루트까지의 서명 체인을 검증하는
방식을 제안한다.

비트 수 논쟁은 위협 모델이 틀렸을 때
의미 없는 최적화가 된다.
"충돌이 일어나면 어쩌지?"보다
"누가 의도적으로 충돌을 만들면 어쩌지?"가
실제 시스템에서 훨씬 중요한 질문이다.

### ID 체계는 조직 구조를 반영한다

Dewey 방식(`13.5.3`)은 계층 조직이다.
중앙 카운터는 중앙집권이다.
Binary는 자원을 균등하게 분배하는 연방제다.
랜덤 ID는 완전한 자율이다.

콘웨이 법칙(Conway's Law)은 시스템 구조가
조직 구조를 반영한다고 말한다.
ID 할당 방식도 마찬가지다.
어떤 ID 체계를 선택하느냐는
시스템이 전제하는 권한 구조와
통신 패턴을 드러낸다.

Dewey가 시뮬레이션에서 가장 좋은 성능을
보인 이유는, Fitness 모델이
인기 있는 노드(위성, 서버)에 요청이 집중되는
현실적 패턴을 반영하기 때문이다.
계층 구조가 실제 통신 패턴과 일치할 때
Dewey는 효율적이다.

하지만 랜덤이 최종적으로 이기는 이유는
우주에 계층이 없기 때문이다.
어떤 노드도 다른 노드의 상위가 아니고,
어떤 행성도 다른 행성의 중앙이 아니다.
권한 구조가 없는 곳에서는
권한 구조를 전제하지 않는 방식이 이긴다.

### 엔트로피 품질이 전제 조건이다

798비트든 122비트든, 랜덤이 진짜 랜덤이
아니면 의미 없다. 의사 난수 생성기(PRNG)의
비랜덤 시드는 충돌 확률을 급격히 높인다.

HN에서 한 사용자는 Intel NIC를 한 상자
받았는데 전부 같은 MAC 주소였다고 한다.
이론적 충돌 확률이 아무리 낮아도,
구현이 잘못되면 충돌은 100%다.

CSPRNG나 양자 난수 소스가 필요하고,
잘 알려진 PRNG의 첫 1,000개 출력값,
전부 0인 ID, 전부 1인 ID 등은
금지 목록에 올려야 한다.

비트 수는 충분 조건이 아니라 필요 조건이다.
충분 조건은 구현의 품질이다.
이론과 실제 사이의 간극은
항상 구현에서 발생한다.

### 테세우스의 배: ID는 무엇을 식별하는가

부품이 하나씩 교체되어 원래 부품이
하나도 남지 않은 우주선은
같은 ID를 유지해야 할까?

원문은 실용적 해법을 제안한다.
ID를 특정 하드웨어 조각에 저장하고,
그 하드웨어가 곧 정체성이라고 정의한다.
무엇에 연결되어 있든 그 칩이 ID다.

또한 같은 행성에 여러 탐사대가 도착하면
각각 다른 ID를 부여할 수 있으므로,
하나의 객체에 여러 ID가 붙을 수 있다.
이 경우 ID 목록을 유지하여
같은 객체를 가리키는 모든 ID를 연결한다.

ID는 "존재"를 가리키는 것이 아니라
"추적 가능한 연속성"을 가리킨다.
데이터베이스에서 Surrogate Key가
자연키보다 선호되는 이유와 같다.
ID는 대상의 본질이 아니라
시스템이 대상을 추적하는 방식이다.

### 문제를 극한까지 밀어보는 가치

UUID 122비트가 충분한지 묻는 질문에
"우주 전체, 열 죽음까지"로 답하는 것은
과잉이다. 실무에서 128비트면 충분하다는 답은
이미 수십 년 전부터 알려져 있었다.

하지만 이 과잉을 통해 나온 통찰들:

결정론적 방식의 최악 선형 성장은
"더 나은 알고리즘"이 없다는 증명이다.
인과적 접촉 논쟁은 "문제의 경계를
먼저 정의하라"는 설계 원칙이다.
Computronium 상한선은 "물리학이 보증하는
충분함"이라는 드문 확신이다.
338KB vs 100바이트는 "확률을 받아들이는 것이
이득"이라는 절충의 정량적 근거다.

실용적 답은 이미 알고 있었지만,
왜 충분한지를 이 수준으로 이해하게 된 것은
극한까지 밀어본 덕분이다.
좋은 사고 실험은 답을 바꾸지 않지만,
답에 대한 확신의 근거를 바꾼다.
